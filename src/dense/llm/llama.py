import os
import tqdm
from openai import OpenAI
from joblib import Memory
from dotenv import load_dotenv
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type,
)

# Load API key from environment variables file
load_dotenv('.env')

# Set up caching
memory = Memory(location=".cache", verbose=0)

# Initialize OpenAI client using the API key for the llama3 model
client = OpenAI(api_key=os.environ.get("DEEP_INF_API_KEY"), base_url=os.environ.get("DEEP_INF_BASE"))

def retry_callback(retry_state):
    """
    Callback function during retries, notifying the user of the retry attempts and exception information.
    """
    print(
        f"Retrying {retry_state.fn.__name__} due to {retry_state.outcome.exception()}."
    )
    print(
        f"Attempt {retry_state.attempt_number} of {retry_state.retry_object.stop.max_attempt_number}."
    )

@retry(
    reraise=True,  # If the retry limit is reached, re-raise the last exception
    stop=stop_after_attempt(8),  # Retry up to 8 times
    wait=wait_exponential(min=2, max=32),  # Exponential backoff strategy, wait time between 2 to 32 seconds
    retry=retry_if_exception_type(Exception),  # Retry for any type of exception
    before_sleep=retry_callback,  # Callback function called before each retry
)
@memory.cache  # Cache the function result to avoid duplicate API calls
def llama3_single_generate(
    input_dict,
    model="meta/llama-3.1-70b-instruct",
    temp=0.0,
    top_p=0.9,
):
    """
    Generate a single response using the llama3 model.

    Args:
        input_dict (Dict[str, str]): A dictionary containing 'system_prompt' and 'user_message'.
        model (str, optional): The name of the model to use. Defaults to "meta/llama-3.1-70b-instruct".
        temp (float, optional): The temperature for generation. Defaults to 0.0.
        top_p (float, optional): The nucleus sampling parameter. Defaults to 0.9.

    Returns:
        str: The response generated by the model.
    """
    
    chat_completion = client.chat.completions.create(
        messages=[
            {
                "role": "system",
                "content": input_dict["system_prompt"],
            },
            {
                "role": "user",
                "content": input_dict["user_message"],
            },
        ],
        model=model,
        temperature=temp,
        top_p=top_p
    )
    return chat_completion.choices[0].message.content

def llama3_generate(
    inputs,
    model="meta/llama-3.1-70b-instruct",
    temp=0.0,
    top_p=0.9,
    mute_tqdm=False,
):
    """
    Generate responses using llama3 for a set of inputs.

    This function iterates over a list of inputs, using the llama3_single_generate function to generate a response for each input.
    Progress is displayed through a tqdm progress bar, which can be muted if desired.

    Args:
        inputs (List[Dict[str, Any]]): A list of input dictionaries containing 'system_prompt' and 'user_message'.
        model (str, optional): The name of the model to use. Defaults to "meta/llama-3.1-70b-instruct".
        temp (float, optional): The temperature for generation. Defaults to 0.0.
        top_p (float, optional): The nucleus sampling parameter. Defaults to 0.9.
        mute_tqdm (bool, optional): Whether to disable the tqdm progress bar. Defaults to False.

    Returns:
        List[str]: A list of responses generated by the model.
    """
    responses = []

    for input_dict in tqdm.tqdm(
        inputs,
        disable=mute_tqdm,
        desc=f"Inference {model}",
        leave=False,
    ):
        response = llama3_single_generate(
            input_dict,
            model=model,
            temp=temp,
            top_p=top_p,
        )
        responses.append(response)

    return responses


if __name__ == "__main__":
    input_list = [
        {
            "system_prompt": "You are a helpful assistant.",
            "user_message": "I want to know about AI.",
        },
        {
            "system_prompt": "You are a helpful assistant.",
            "user_message": "Tell me about climate change.",
        },
    ]
    responses = llama3_generate(input_list)
    print(responses)
