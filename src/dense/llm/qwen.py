import os
import tqdm
from openai import OpenAI
from joblib import Memory
from dotenv import load_dotenv
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type,
)

# Load API keys from the environment variable file
load_dotenv('.env')

# Set up caching
memory = Memory(location=".cache", verbose=0)

# Initialize the OpenAI client using the API key for the qwen2.5-7b-instruct model
client = OpenAI(api_key=os.environ.get("YOUR_BAILIAN_API_KEY"), base_url=os.environ.get("YOUR_BAILIAN_BASE_URL"))

def retry_callback(retry_state):
    """
    Callback function during retries to notify the user about the retry attempt and exception information.
    """
    print(
        f"Retrying {retry_state.fn.__name__} due to {retry_state.outcome.exception()}."
    )
    print(
        f"Attempt {retry_state.attempt_number} of {retry_state.retry_object.stop.max_attempt_number}."
    )

@retry(
    reraise=True,  # Re-raise the last exception if the retry attempts are exhausted
    stop=stop_after_attempt(8),  # Retry a maximum of 8 times
    wait=wait_exponential(min=2, max=32),  # Exponential backoff strategy with wait times between 2 to 32 seconds
    retry=retry_if_exception_type(Exception),  # Retry on any type of exception
    before_sleep=retry_callback,  # Callback function to call before each retry
)
@memory.cache  # Cache the function results to avoid redundant API calls
def qwen_single_generate(
    input_dict,
    model="qwen2.5-7b-instruct",
    temp=0.0,
    top_p=0.9,
):
    """
    Generate a single response using the qwen2.5-7b-instruct model.

    Args:
        input_dict (Dict[str, str]): A dictionary containing 'system_prompt' and 'user_message'.
        model (str, optional): The name of the model to use. Defaults to "qwen2.5-7b-instruct".
        temp (float, optional): The temperature for generation. Defaults to 0.0.
        top_p (float, optional): The nucleus sampling parameter. Defaults to 0.9.

    Returns:
        str: The response generated by the model.
    """
    chat_completion = client.chat.completions.create(
        messages=[
            {
                "role": "system",
                "content": input_dict["system_prompt"],
            },
            {
                "role": "user",
                "content": input_dict["user_message"],
            },
        ],
        model=model,
        temperature=temp,
        top_p=top_p
    )
    return chat_completion.choices[0].message.content

def qwen_generate(
    inputs,
    model="qwen2.5-7b-instruct",
    temp=0.0,
    top_p=0.9,
    mute_tqdm=False,
):
    """
    Generate responses from qwen2.5-7b-instruct for a set of inputs.

    This function iterates over a list of inputs and uses the `qwen_single_generate` function to generate a response for each input.
    Progress is displayed using a tqdm progress bar, which can be optionally muted.

    Args:
        inputs (List[Dict[str, Any]]): A list of input dictionaries containing 'system_prompt' and 'user_message'.
        model (str, optional): The name of the model to use. Defaults to "qwen2.5-7b-instruct".
        temp (float, optional): The temperature for generation. Defaults to 0.0.
        top_p (float, optional): The nucleus sampling parameter. Defaults to 0.9.
        mute_tqdm (bool, optional): Whether to disable the tqdm progress bar. Defaults to False.

    Returns:
        List[str]: A list of responses generated by the model.
    """
    responses = []

    for input_dict in tqdm.tqdm(
        inputs,
        disable=mute_tqdm,
        desc=f"Inference {model}",
        leave=False,
    ):
        response = qwen_single_generate(
            input_dict,
            model=model,
            temp=temp,
            top_p=top_p,
        )
        responses.append(response)

    return responses


if __name__ == "__main__":
    input_list = [
        {
            "system_prompt": "You are a helpful assistant.",
            "user_message": "I want to know about AI.",
        },
        {
            "system_prompt": "You are a helpful assistant.",
            "user_message": "Tell me about climate change.",
        },
    ]
    responses = qwen_generate(input_list)
    print(responses)
